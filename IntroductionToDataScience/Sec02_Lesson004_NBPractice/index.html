
      <html>
        <head>
            <style>
      body {
        font-size: 10pt;
        margin: 1.5em;
        background-color: lightblue;
        color: darkblue;
        font-family: Verdana,sans-serif;
      }
      h1 {
        font-size: 1.2em;
        font-weight: bold;
        margin-top: 2em;
      }
      h2 {
        font-size: 1.1em;
        font-weight: bold;
      }
      fieldset {
        width: 740px;
        margin-bottom: 12px;
        border-color: #00457b;
        background-color: #cfeace;
      }

      fieldset div {
        margin-bottom: 6px;
        font-weight: normal;
      }

      legend {
        border: 2px ridge #00457b;
        font-size: 1.2em;
        font-weight: bold;
        background-color: #e36a51;
        color: white;
        padding: 8px 16px;
      }
    </style>
        </head>
        <body>
      <fieldset><legend>./IntroductionToDataScience/Sec02_Lesson004_NBPractice</legend>
        <p><h2><a href="https://winvector.github.io/IntroductionToDataScience/index.html">..</a></h2></p>
         <ul>

<pre>
Shakespeare: from http://www.gutenberg.org/ebooks/100

Classifying author by naive Bayes over 2-gram bag of words.
The example is just notional.  In practice you would want more
features (NLP, word2vec), feature pruning (remove proper names) and more sophisticated statistics (author-topic modeling, latent Dirichlet allocation).

</pre>
          <li><a href="NBShakespeare.Rmd">NBShakespeare.Rmd</a></li>
          <li><a href="NBShakespeare.html">NBShakespeare.html</a></li>
          <li><a href="README.txt">README.txt</a></li>
          <li><a href="pg1524.txt.gz">pg1524.txt.gz</a></li>
          <li><a href="pg20288.txt.gz">pg20288.txt.gz</a></li>
          <li><a href="pg2264.txt.gz">pg2264.txt.gz</a></li>
          <li><a href="pg811.txt.gz">pg811.txt.gz</a></li>
         </ul>
        </fieldset></body>
      </html>
